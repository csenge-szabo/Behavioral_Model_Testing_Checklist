# Behavioral Testing of SRL Models using Checklist

### Author:
Csenge Szab√≥

### Introduction
The aim of this project is to implement Checklist methods for testing SRL models created throughout the Advanced NLP Master's course at VU Amsterdam. The aim of this individual assignment is to challenge transformer-based models in their capabilities of Semantic Role Labeling.

For the full project report and the results, read `Challenging_SRL_Models_Csenge_Szabo.pdf`.

### Data: 
For this project the fine-tuned DistilBert models were trained using the Universal Proposition Banks version 1.0 [https://universalpropositions.github.io] for English language, which was created with the aim to study Semantic Role Labelling (SRL). The aim of this work is to explore and evaluate the models' performance on SRL using CheckList tests. The test sets stored in JSON files are partially hand-crafted and partially created using ChatGPT4.

### Steps before experiments: 
- Ensure all dependencies listed in the 'requirements.txt' are installed in your environment.
- Download the trained models from this link: 
https://www.dropbox.com/scl/fo/xv6pkmvqfs4eaptr0aw9i/h?rlkey=jk8ggqbkrngjclxduq2fod3dy&dl=0
- Place and unzip them in the 'models' directory for usage in predictions.

### Files: 
Test sets:
- Seven main capabilities are tested in this project, these are organised in corresponding JSON files representing different capabilities for SRL.
BERT input files:
- The test instances are organised into the respective directories 'input_M1','input_M2', and 'input_M3' for each model variant. These directories contain CONLLU formatted files ready for model prediction.
BERT output files:
- The predictions generated by each model for every test type are organised in the corresponding directories 'output_M1','output_M2', and 'output_M3'.

### Scripts: 
Preprocessing:
- preprocessing.py is responsible for transforming the JSON test sets into CONLLU formatted files suitable for DistilBERT model predictions. This script ensures that the test instances are properly formatted and stored in separate files for each unique test type within their respective model input directories.
- Usage: python preprocessing.py

Prediction with the models:
- prediction.py executes all three fine-tuned DistilBERT models on their respective input CONLLU files. It writes the predictions in TSV format and organizes these files into the appropriate output directories for each model.
- This script ensures predictions are generated and stored systematically for further evaluation.
- Usage: python prediction.py

Evaluation:
- evaluation.py script is designed to assess the performance of each model's predictions against the gold annotations. It uses three distinct evaluation functions to assess the models on Minimum Functionality Tests (MFT), Invariance Tests (INV), and Directionality Tests (DIR) separately.
- The evaluation output includes the failure rate and the sentence IDs of failed test instances for each file.
- The script automatically recognizes the test type contained within each file based on the file name (MFT/INV/DIR) and evaluates the test accordingly.
- Usage: python evaluation.py








